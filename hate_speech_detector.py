# -*- coding: utf-8 -*-
"""hate speech detector

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nEdYJpNr0_eKYqjiNre3-9TIJsogOPW0
"""

import pandas as pd
import numpy as np
import re
from nltk.tokenize import TweetTokenizer
from nltk.stem import PorterStemmer
import string
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
import nltk
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from keras.utils import to_categorical

df=pd.read_csv('/content/hate speech.csv')

df.head()

df.info()

df['class'].isnull().sum()

df['tweet'].isnull().sum()

#0: "Hate Speech", 1: "Offensive Language", 2: "normal"

df.shape

data = df[['tweet', 'class']]

data.head()

x_train,x_test,y_train,y_test=train_test_split(data['tweet'],data['class'],test_size=0.2,random_state=42)

print('shape of x_train:',x_train.shape)
print("shape of x_test:", x_test.shape)
print('shape of y_train:', y_train.shape)
print("shape of y_test:", y_test.shape)

tweets_list=x_train.astype(str).tolist()

prediction_list=x_test.astype(str).tolist()

X_initial=np.zeros((x_train.shape[0],4))

labels_list=y_train.tolist()

validation_list=y_test.tolist()

"""#preprocessing"""

print(tweets_list)

class TweetCleaner(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.hashtags = r'#'
        self.links = r'https?:\/\/.*[\r\n]*'
        self.digits = r'\d+'
        self.retweet = r'^RT[\s]+'
        self.username = r'@\w+'
        self.cleaning_patterns = [self.hashtags, self.links, self.digits, self.retweet, self.username]

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        cleaned_tweets = []
        for tweet in X:
            cleaned_tweet = tweet
            for pattern in self.cleaning_patterns:
                cleaned_tweet = re.sub(pattern, '', cleaned_tweet)
            cleaned_tweets.append(cleaned_tweet)
        return cleaned_tweets

"""class check"""

tweetcleaner_object=TweetCleaner()

copy_list=tweets_list.copy()
cleaned_tweets=tweetcleaner_object.fit_transform(copy_list)

print(cleaned_tweets)

"""1. tokenization
2. lower case
3. stopwords and punctations
4. stemming






"""

class TweetTokenizerTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        tokenized_tweets = []
        for tweet in X:
            tokenized_tweet = self.tokenizer.tokenize(tweet)
            tokenized_tweets.append(tokenized_tweet)
        return tokenized_tweets

"""class check"""

tokenizer_object=TweetTokenizerTransformer()

output_tweets=tokenizer_object.fit_transform(copy_list)

print(output_tweets)

"""downloading stopwords"""

nltk.download('stopwords')

class StopwordsPunctuationRemover(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.punctuation = set(string.punctuation)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        cleaned_tweets = []
        for tokens in X:
            cleaned_tokens = [token for token in tokens if token not in self.stop_words and token not in self.punctuation]
            cleaned_tweets.append(cleaned_tokens)
        return cleaned_tweets

"""class check"""

stopword_object=StopwordsPunctuationRemover()

output_tweets=stopword_object.fit_transform(copy_list)

"""stemming"""

class TweetStemmer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.pstem = PorterStemmer()

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        stemmed_tweets = []
        for tokens in X:
            stemmed_tokens = [self.pstem.stem(token) for token in tokens]
            stemmed_tweets.append(stemmed_tokens)
        return stemmed_tweets

"""# pipeline"""

pline=Pipeline([('cleaner',TweetCleaner()),
                ('tokenizer',TweetTokenizerTransformer()),
                ('stopword',StopwordsPunctuationRemover()),
                ('stemmer',TweetStemmer()),
                ])

stemmed_tweets=pline.fit_transform(tweets_list)

predicting_tweets=pline.fit_transform(prediction_list)

print(stemmed_tweets)

print(predicting_tweets)

"""#feature extraction"""

count_labels=0
count_final=0
for i in labels_list:
  count_labels+=1
for j in stemmed_tweets:
  count_final+=1
print(count_labels , count_final)

freq={}
for y,tokens in zip(labels_list,stemmed_tweets):
    for token in tokens:
         pair=(token,y)
         if pair in freq:
          freq[pair]+=1
         else:
          freq[pair]=1

print(freq)

def extract_feature(tweets_list,freq,X):
    X[:,0]=1
    for i, tweet in enumerate(tweets_list):
        for token in tweet:
            X[i, 1] += freq.get((token, "Hate Speech"), 0)
            X[i, 2] += freq.get((token, "Offensive Language"), 0)
            X[i, 3] += freq.get((token, "normal"), 0)

    return X

X_final=extract_feature(stemmed_tweets,freq,X_initial)

X_val=extract_feature(predicting_tweets,freq,np.zeros((x_test.shape[0],4)))

print(X_val)

print(X_final)

"""# Neaural Network"""

X_final.shape

len(labels_list)

X_val.shape

len(validation_list)

labels_array = np.array(labels_list)
validation_array = np.array(validation_list)
labels_one_hot = to_categorical(labels_array, num_classes=3)
validation_one_hot = to_categorical(validation_array, num_classes=3)

model = Sequential()
model.add(Dense(32, activation='sigmoid', input_shape=(4,)))
model.add(Dense(16, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.summary()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_final, labels_one_hot, epochs=15, batch_size=32, validation_data=(X_val,validation_one_hot))

val_loss, val_accuracy = model.evaluate(X_val, validation_one_hot)
print(f"Validation Loss: {val_loss}")
print(f"Validation Accuracy: {val_accuracy}")

sample=df['tweet']
print(sample)

random_tweet_index = np.random.randint(len(sample))
random_tweet = sample.iloc[random_tweet_index]
input_data = pline.transform([random_tweet])

print(input_data)

input_value=extract_feature(input_data,freq,np.zeros((len(input_data), 4)))

reshaped_input = input_value.reshape(1, -1)

reshaped_input.shape

prediction = model.predict(reshaped_input)

predicted_class = np.argmax(prediction)
print(f"The predicted class is: {predicted_class}")

"""0: "Hate Speech", 1: "Offensive Language", 2: "normal"
"""

